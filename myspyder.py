import cfg
from bs4 import BeautifulSoup
import requests
import re
import datetime
from pymongo import MongoClient
from requests import exceptions
import myutils

class spyder:
    def connectDb(self):
        """
            # Function To Make Connection To Database
            Returns : Collection
        """
        try:
            client = MongoClient()
            db = client[cfg.db_name]
            table = db[cfg.db_collection]
            return table
        except Exception as inst:
            print("Db connection Error",inst.args)
    
    def handleExceptions(self, inst, table, src):
        """
            # Function To Handle Exceptions Generated by request
        """
        try:
            requests.get(cfg.source)
        except:
            table.update_one({"Link":src},{"$set":{
                    "IsCrawled":False
            }})
            pass
    
    def getnextlink(self,table):
        """
            # Function To Get next obj which is to be scrawled
            Returns : Json Object Containing information about Link 
        """
        now = datetime.timedelta(cfg.maxTime)
        delta = datetime.datetime.utcnow() - now
        obj = table.find_one({"IsCrawled":False})
        if obj is not None:
            table.update_one({"Link":obj["Link"]},{"$set":{
                    "IsCrawled":True
            }})
            return obj
        obj = table.find_one({"LastCrawlDate": {"$lt":delta}})
        if obj is not None:
            table.update_one({"Link":obj["Link"]},{"$set":{
                    "LastCrawlDate":datetime.datetime.utcnow()
            }})
            return obj
        return obj

    def getlinks(self,obj,table):
        """
            # Function To get all links present in current page and insert in to Database
        """
        src = obj['Link']
        fn = obj['Filepath']
        try:
            r = requests.get(src)
        except Exception as inst:
            self.handleExceptions(inst,table,src)
            return
        if r.status_code==200:  #if connection is successful
            myutils.writetofile(fn,r) #write page to file
            myutils.extractPage(r,src,table) #Extract all the links
        table.update_one({"Link":src},{"$set":{
                "IsCrawled":True,
                "LastCrawlDate":datetime.datetime.utcnow(),
                "ResponseStatus":r.status_code,
                "Contenttype":r.headers.get('Content-Type'),
                "ContentLength": len(r.content),
                "Filepath":fn
        }})
    
    
